{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "https://github.com/takky0330/wisper/blob/main/intervviewZero.ipynb",
      "authorship_tag": "ABX9TyN0NLPNgD/ozUYO0o8/D+zB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takky0330/wisper/blob/main/intervviewZero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "N_oGHqinTJxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb88215-09af-41bf-ec99-d1315f038d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/jianfch/stable-ts.git\n",
            "  Cloning https://github.com/jianfch/stable-ts.git to /tmp/pip-req-build-7ce31j8h\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/jianfch/stable-ts.git /tmp/pip-req-build-7ce31j8h\n",
            "  Resolved https://github.com/jianfch/stable-ts.git to commit 7dab1714c5915e3efe070ae697ade1d04f5c8c42\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.17.5) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.17.5) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.17.5) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.17.5) (4.66.6)\n",
            "Requirement already satisfied: openai-whisper<=20240930,>=20230314 in /usr/local/lib/python3.10/dist-packages (from stable-ts==2.17.5) (20240930)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper<=20240930,>=20230314->stable-ts==2.17.5) (0.60.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper<=20240930,>=20230314->stable-ts==2.17.5) (10.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper<=20240930,>=20230314->stable-ts==2.17.5) (0.8.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper<=20240930,>=20230314->stable-ts==2.17.5) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.17.5) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.17.5) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.17.5) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.17.5) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.17.5) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->stable-ts==2.17.5) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->stable-ts==2.17.5) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->stable-ts==2.17.5) (3.0.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper<=20240930,>=20230314->stable-ts==2.17.5) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper<=20240930,>=20230314->stable-ts==2.17.5) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper<=20240930,>=20230314->stable-ts==2.17.5) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper<=20240930,>=20230314->stable-ts==2.17.5) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper<=20240930,>=20230314->stable-ts==2.17.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper<=20240930,>=20230314->stable-ts==2.17.5) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper<=20240930,>=20230314->stable-ts==2.17.5) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "#!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install -U git+https://github.com/jianfch/stable-ts.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install moviepy"
      ],
      "metadata": {
        "id": "iLJk39Pl9X3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc42f0ef-64c3-4f1e-9545-cf35717905c2"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.26.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.36.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (75.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ffmpeg-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT_AwBERdCH-",
        "outputId": "d2b6fa64-7386-41e1-cf38-e472f3fbdbe5"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import whisper\n",
        "import stable_whisper\n",
        "import ffmpeg\n",
        "import numpy as np\n",
        "import json, re, os\n",
        "import glob\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "from moviepy.editor import VideoFileClip\n",
        "from moviepy.editor import AudioFileClip"
      ],
      "metadata": {
        "id": "FBaUgrssTrus"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ./drive/MyDrive/IZ-2218確認４/*.* ./"
      ],
      "metadata": {
        "id": "8ZZr3sOuBEDU"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_webm(path):\n",
        "    with open('./' + path) as f:\n",
        "        for _line in f:\n",
        "            if _line.rstrip()[-5:] == '.webm':\n",
        "                return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "YiH52CTdeaai"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_webm(input_file):\n",
        "    #!ffmpeg -allowed_extensions ALL -f hls -i ./drive/MyDrive/IZ-2218確認３/ee93aa2994411ebff28d459c4711228b_1731555758981__uid_s_907719631__uid_e_video.m3u8 -c:v copy -y ./ee93aa2994411ebff28d459c4711228b_1731555758981__uid_s_907719631__uid_e_video.webm\n",
        "    webm_tmp = \"./video_tmp.webm\"\n",
        "    try:\n",
        "        (\n",
        "            ffmpeg\n",
        "            .input('./' + input_file, allowed_extensions='ALL')\n",
        "            .output(webm_tmp, vcodec=\"copy\")\n",
        "            .overwrite_output()\n",
        "            .run(capture_stderr=True, capture_stdout=True)\n",
        "        )\n",
        "        _clip = VideoFileClip(webm_tmp)\n",
        "    except ffmpeg.Error as e:\n",
        "        #print(f\"{e.stderr.decode()}\")\n",
        "        #print(f\"標準出力: {e.stdout.decode()}\")\n",
        "        _clip = None\n",
        "    return _clip"
      ],
      "metadata": {
        "id": "9o52BsqOfiQV"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_list(path):\n",
        "    video_path = f'{path}*_video.m3u8'\n",
        "    video_paths = glob.glob(video_path)\n",
        "    video_paths.sort()\n",
        "    return video_paths\n",
        "def get_audio_list(path):\n",
        "    audio_path = f'{path}*_audio.m3u8'\n",
        "    audio_paths = glob.glob(audio_path)\n",
        "    audio_paths.sort()\n",
        "    return audio_paths"
      ],
      "metadata": {
        "id": "4DqRD33Bh4yy"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = './'\n",
        "video_paths = get_video_list(path)\n",
        "audio_paths = get_audio_list(path)"
      ],
      "metadata": {
        "id": "XT68uSsBMEwJ"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video = []\n",
        "for video_path in video_paths:\n",
        "    _video = conv_webm(video_path) if is_webm(video_path) else VideoFileClip(video_path)\n",
        "    video.append(_video)\n",
        "print(video)"
      ],
      "metadata": {
        "id": "p2r1n94mMKzy",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16722619-4a35-4501-b486-c6985f02a644"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<moviepy.video.io.VideoFileClip.VideoFileClip object at 0x7813903534f0>, <moviepy.video.io.VideoFileClip.VideoFileClip object at 0x781390435d20>, <moviepy.video.io.VideoFileClip.VideoFileClip object at 0x7813904edd20>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio = []\n",
        "wav_paths = []\n",
        "for audio_path in audio_paths:\n",
        "    wav_path = '.'.join(audio_path.split('.')[:-1]) + '.wav'\n",
        "    audio_tmp = AudioFileClip(audio_path)\n",
        "    audio_tmp.write_audiofile(wav_path, ffmpeg_params=[\"-ac\", \"1\"])\n",
        "    audio.append(audio_tmp)\n",
        "    wav_paths.append(wav_path)\n",
        "print(audio)"
      ],
      "metadata": {
        "id": "3JW5DgVSK5AU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c3f1c2-8cda-4443-d814-a6c87d0da22b"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in ./ece061b98f490713b71deabf1c7df387_1731585061833__uid_s_186868730__uid_e_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "MoviePy - Writing audio in ./ece061b98f490713b71deabf1c7df387_1731585061833__uid_s_801600513__uid_e_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "MoviePy - Writing audio in ./ece061b98f490713b71deabf1c7df387_1731585061833__uid_s_846732376__uid_e_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "[<moviepy.audio.io.AudioFileClip.AudioFileClip object at 0x781377f984f0>, <moviepy.audio.io.AudioFileClip.AudioFileClip object at 0x78137c1df5e0>, <moviepy.audio.io.AudioFileClip.AudioFileClip object at 0x7813904eeb90>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YyZDL6H4Nl7L"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = whisper.load_model(\"large\")\n",
        "model = stable_whisper.load_model('large')\n",
        "stable_whisper.modify_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "tQ_TEOh7swqx",
        "outputId": "1fc7e961-8278-48df-c14a-713dfd70f869"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 254.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 35.06 MiB is free. Process 17269 has 14.71 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 396.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-167-4e75d04afe37>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model = whisper.load_model(\"large\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstable_whisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'large'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstable_whisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodify_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_whisper/whisper_word_level/original_whisper.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory, cpu_preload, dq, engine)\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcuda_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcuda_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \"\"\"\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \"\"\"\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 254.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 35.06 MiB is free. Process 17269 has 14.71 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 396.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asr_result = []\n",
        "for wav_patn in wav_paths:\n",
        "    print(f'\\n---- {wav_patn} ----')\n",
        "    _result = model.transcribe(wav_patn, verbose=True, fp16=False, language=\"ja\", word_timestamps=True)\n",
        "    asr_result.append(_result)"
      ],
      "metadata": {
        "id": "v-IhvxuSHL72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEG = []\n",
        "for spk, res in enumerate(asr_result):\n",
        "    for seg in res.segments:\n",
        "        SEG.append([seg.start, seg.end, seg.text, f'speaker_{format(spk, \"02\") }'])"
      ],
      "metadata": {
        "id": "e-ve0CjbIy0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEG_SORTED = sorted(SEG, key=lambda x:(x[0], x[1]))\n",
        "SEG_SORTED\n"
      ],
      "metadata": {
        "id": "BhIidIwe7kwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEG_SORTED"
      ],
      "metadata": {
        "id": "tBTMkyX7OH5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(int)"
      ],
      "metadata": {
        "id": "j-CY_JiTWSp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prog = re.compile('教えて|聞か|聞き|願い|ください|ですか(?!ね)|でしょうか(?!ね)|ましたか|されましたか|いつ頃')\n",
        "INTERVIEWERE = {}\n",
        "INT_COUNT = []\n",
        "for c in range(len(asr_result)):\n",
        "    INT_COUNT.append([0, 0, 0, ''])\n",
        "for srt, end, txt, spk in SEG_SORTED:\n",
        "    count = len(prog.findall(txt))\n",
        "    spk_no = int(spk.split('_')[-1])\n",
        "    INT_COUNT[spk_no][0] += 1\n",
        "    INT_COUNT[spk_no][3] = spk\n",
        "    if count > 0:\n",
        "        INT_COUNT[spk_no][1] += count\n",
        "for c, cnt in enumerate(INT_COUNT):\n",
        "    INT_COUNT[c][2] = cnt[1] / cnt[0]\n",
        "INT_COUNT.index(min(INT_COUNT))\n",
        "\n",
        "rate = [c[2] for c in INT_COUNT]\n",
        "min_index = rate.index(min(rate))\n",
        "\n",
        "for i, intv in enumerate(INT_COUNT):\n",
        "    if i == min_index:\n",
        "        INTERVIEWERE[intv[3]] = \"モニター\"\n",
        "    else:\n",
        "        INTERVIEWERE[intv[3]] = \"インタビューア\" + intv[3].split('_')[-1]\n",
        "print(INTERVIEWERE)"
      ],
      "metadata": {
        "id": "1TYp74Tq4LTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#INT_COUNT = [[0] * 3] * len(asr_result)\n",
        "#INT_COUNT = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
        "#INT_COUNT = [[0, 0, 0]] * len(asr_result)\n",
        "INT_COUNT = []\n",
        "for c in range(len(asr_result)):\n",
        "    INT_COUNT.append([0, 0, 0])\n",
        "INT_COUNT[2][2] = 1\n",
        "print(INT_COUNT[0][0])\n",
        "print(INT_COUNT[0][1])\n",
        "print(INT_COUNT[0][2])\n",
        "print(INT_COUNT[1][0])\n",
        "print(INT_COUNT[1][1])\n",
        "print(INT_COUNT[1][2])\n",
        "print(INT_COUNT[2][0])\n",
        "print(INT_COUNT[2][1])\n",
        "print(INT_COUNT[2][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCsI07wu-2nc",
        "outputId": "f6f6efce-6f42-4ba3-d809-611819034408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "du7yMlxqsi-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_result = {}\n",
        "whisper_segment = []\n",
        "for asr in asr_result['segments']:\n",
        "    if asr['end'] - asr['start'] >= 0.6 and len(asr['text']) >= 2:\n",
        "        asr['text'] += '。'\n",
        "        whisper_segment.append(asr)\n",
        "whisper_result['segments'] = whisper_segment"
      ],
      "metadata": {
        "id": "HI_bYFi2kFcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_result"
      ],
      "metadata": {
        "id": "Zq59lbJNnJ0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#speaker_count = 2\n",
        "speaker_count = 3\n",
        "\n",
        "import torch\n",
        "pipeline.to(torch.device(\"cuda\"))\n",
        "diarization_result = pipeline(audio_path, num_speakers=speaker_count)\n",
        "#diarization_result = pipeline(audio_path, min_speakers=2, max_speakers=5)"
      ],
      "metadata": {
        "id": "4FVUQIrkXKEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diarization_result"
      ],
      "metadata": {
        "id": "9RsGrxvMw-xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final_result = diarize_text(asr_result, diarization_result)\n",
        "final_result = diarize_text(whisper_result, diarization_result)"
      ],
      "metadata": {
        "id": "WETpYo84XUGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prog = re.compile('教えて|聞か|聞き|願い|ください|ですか|でしょうか|ましたか|されましたか|いつ頃')\n",
        "INTERVIEW = []\n",
        "INT_COUNT = []\n",
        "for seg, spk, sent in final_result:\n",
        "  if spk is None:\n",
        "    spk = INTERVIEW[-1]['spk']\n",
        "  count = len(prog.findall(sent))\n",
        "  spk_no = int(spk.split('_')[-1])\n",
        "  if count > 0: print(str(sent))\n",
        "  _seg = {'start': round(seg.start, 2), 'end': round(seg.end, 2), 'text': sent, 'count': count, 'spk': spk, 'spk_no': spk_no}\n",
        "  INTERVIEW.append(_seg)\n",
        "  INT_COUNT.append(int(spk[-1]))\n",
        "\n",
        "speaker_count = max(INT_COUNT) + 1\n",
        "\n",
        "speek = [0] * speaker_count\n",
        "for INTV in INTERVIEW:\n",
        "    speek[INTV[\"spk_no\"]] += INTV[\"count\"]\n",
        "inteviewer_index = np.argmax(speek)\n",
        "monitor_index = np.argmin(speek)\n",
        "\n",
        "print(speek)\n",
        "print(f'インタービューア ： SPEAKER_0{inteviewer_index}')\n",
        "print(f'モニター　　　　 ： SPEAKER_0{monitor_index}')"
      ],
      "metadata": {
        "id": "g7vH5Jh0Qmgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seg, spk, sent in final_result:\n",
        "    speeker_name = spk\n",
        "    #speeker_name =   'モニタ　　　　'\n",
        "    #if spk == 'SPEAKER_0' + str(inteviewer_index):\n",
        "    #  speeker_name = 'インタビューア'\n",
        "    line = f'{seg.start:.2f}s~{seg.end:.2f}s {speeker_name} {sent}'\n",
        "    print(line)"
      ],
      "metadata": {
        "id": "7s_fnLsNQj8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "InterReview_json = {\"movie_name\": video_name}\n",
        "speak_json = []\n",
        "for seg, spk, sent in final_result:\n",
        "    _speak = {\"start\": f'{seg.start:.2f}s', \"end\": f'{seg.end:.2f}s', 'speaker': spk, 'speak': sent}\n",
        "    speak_json.append(_speak)\n",
        "InterReview_json['InterReview'] = speak_json\n",
        "\n",
        "json_path = '.'.join(video_path.split('.')[:-1]) + '.json'\n",
        "with open(json_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(InterReview_json, f, indent=4, ensure_ascii=False)\n"
      ],
      "metadata": {
        "id": "Q3H660Ljx-XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alh ./"
      ],
      "metadata": {
        "id": "JVo964bKAgPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ./InterviewZero_0_audio_mask.json ./drive/MyDrive"
      ],
      "metadata": {
        "id": "Reu1aUW8_iX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ./InterviewZero_1_audio_mask.json ./drive/MyDrive"
      ],
      "metadata": {
        "id": "UUx_tPR-Brjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ./InterviewZero_2_audio_mask.json ./drive/MyDrive"
      ],
      "metadata": {
        "id": "I5D_hrldBsLm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}